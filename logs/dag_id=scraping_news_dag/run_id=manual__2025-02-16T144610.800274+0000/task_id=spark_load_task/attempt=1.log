[2025-02-16T14:48:48.996+0000] {taskinstance.py:1979} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: scraping_news_dag.spark_load_task manual__2025-02-16T14:46:10.800274+00:00 [queued]>
[2025-02-16T14:48:49.004+0000] {taskinstance.py:1979} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: scraping_news_dag.spark_load_task manual__2025-02-16T14:46:10.800274+00:00 [queued]>
[2025-02-16T14:48:49.004+0000] {taskinstance.py:2193} INFO - Starting attempt 1 of 1
[2025-02-16T14:48:49.016+0000] {taskinstance.py:2214} INFO - Executing <Task(SparkSubmitOperator): spark_load_task> on 2025-02-16 14:46:10.800274+00:00
[2025-02-16T14:48:49.021+0000] {standard_task_runner.py:60} INFO - Started process 3487 to run task
[2025-02-16T14:48:49.023+0000] {standard_task_runner.py:87} INFO - Running: ['airflow', 'tasks', 'run', 'scraping_news_dag', 'spark_load_task', 'manual__2025-02-16T14:46:10.800274+00:00', '--job-id', '98', '--raw', '--subdir', 'DAGS_FOLDER/scraping-news-dag.py', '--cfg-path', '/tmp/tmpbi6ufmom']
[2025-02-16T14:48:49.025+0000] {standard_task_runner.py:88} INFO - Job 98: Subtask spark_load_task
[2025-02-16T14:48:49.038+0000] {logging_mixin.py:188} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/settings.py:194 DeprecationWarning: The sql_alchemy_conn option in [core] has been moved to the sql_alchemy_conn option in [database] - the old setting has been used, but please update your config.
[2025-02-16T14:48:49.066+0000] {task_command.py:423} INFO - Running <TaskInstance: scraping_news_dag.spark_load_task manual__2025-02-16T14:46:10.800274+00:00 [running]> on host deproject-airflow-scheduler
[2025-02-16T14:48:49.136+0000] {taskinstance.py:2510} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='scraping_news_dag' AIRFLOW_CTX_TASK_ID='spark_load_task' AIRFLOW_CTX_EXECUTION_DATE='2025-02-16T14:46:10.800274+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2025-02-16T14:46:10.800274+00:00'
[2025-02-16T14:48:49.144+0000] {base.py:83} INFO - Using connection ID 'spark_main' for task execution.
[2025-02-16T14:48:49.145+0000] {spark_submit.py:344} INFO - Spark-Submit cmd: spark-submit --master spark://deproject-spark-master:7077 --name arrow-spark /spark-scripts/spark-news.py
[2025-02-16T14:48:51.098+0000] {spark_submit.py:495} INFO - 25/02/16 14:48:51 INFO SparkContext: Running Spark version 3.3.2
[2025-02-16T14:48:51.137+0000] {spark_submit.py:495} INFO - 25/02/16 14:48:51 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2025-02-16T14:48:51.201+0000] {spark_submit.py:495} INFO - 25/02/16 14:48:51 INFO ResourceUtils: ==============================================================
[2025-02-16T14:48:51.201+0000] {spark_submit.py:495} INFO - 25/02/16 14:48:51 INFO ResourceUtils: No custom resources configured for spark.driver.
[2025-02-16T14:48:51.202+0000] {spark_submit.py:495} INFO - 25/02/16 14:48:51 INFO ResourceUtils: ==============================================================
[2025-02-16T14:48:51.202+0000] {spark_submit.py:495} INFO - 25/02/16 14:48:51 INFO SparkContext: Submitted application: FinalProject
[2025-02-16T14:48:51.215+0000] {spark_submit.py:495} INFO - 25/02/16 14:48:51 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
[2025-02-16T14:48:51.222+0000] {spark_submit.py:495} INFO - 25/02/16 14:48:51 INFO ResourceProfile: Limiting resource is cpu
[2025-02-16T14:48:51.222+0000] {spark_submit.py:495} INFO - 25/02/16 14:48:51 INFO ResourceProfileManager: Added ResourceProfile id: 0
[2025-02-16T14:48:51.254+0000] {spark_submit.py:495} INFO - 25/02/16 14:48:51 INFO SecurityManager: Changing view acls to: airflow
[2025-02-16T14:48:51.254+0000] {spark_submit.py:495} INFO - 25/02/16 14:48:51 INFO SecurityManager: Changing modify acls to: airflow
[2025-02-16T14:48:51.255+0000] {spark_submit.py:495} INFO - 25/02/16 14:48:51 INFO SecurityManager: Changing view acls groups to:
[2025-02-16T14:48:51.255+0000] {spark_submit.py:495} INFO - 25/02/16 14:48:51 INFO SecurityManager: Changing modify acls groups to:
[2025-02-16T14:48:51.256+0000] {spark_submit.py:495} INFO - 25/02/16 14:48:51 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(airflow); groups with view permissions: Set(); users  with modify permissions: Set(airflow); groups with modify permissions: Set()
[2025-02-16T14:48:51.424+0000] {spark_submit.py:495} INFO - 25/02/16 14:48:51 INFO Utils: Successfully started service 'sparkDriver' on port 41739.
[2025-02-16T14:48:51.446+0000] {spark_submit.py:495} INFO - 25/02/16 14:48:51 INFO SparkEnv: Registering MapOutputTracker
[2025-02-16T14:48:51.475+0000] {spark_submit.py:495} INFO - 25/02/16 14:48:51 INFO SparkEnv: Registering BlockManagerMaster
[2025-02-16T14:48:51.491+0000] {spark_submit.py:495} INFO - 25/02/16 14:48:51 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[2025-02-16T14:48:51.492+0000] {spark_submit.py:495} INFO - 25/02/16 14:48:51 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
[2025-02-16T14:48:51.497+0000] {spark_submit.py:495} INFO - 25/02/16 14:48:51 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
[2025-02-16T14:48:51.514+0000] {spark_submit.py:495} INFO - 25/02/16 14:48:51 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-57580d27-9560-4153-8e03-2522b3280b2d
[2025-02-16T14:48:51.534+0000] {spark_submit.py:495} INFO - 25/02/16 14:48:51 INFO MemoryStore: MemoryStore started with capacity 434.4 MiB
[2025-02-16T14:48:51.549+0000] {spark_submit.py:495} INFO - 25/02/16 14:48:51 INFO SparkEnv: Registering OutputCommitCoordinator
[2025-02-16T14:48:51.703+0000] {spark_submit.py:495} INFO - 25/02/16 14:48:51 INFO Utils: Successfully started service 'SparkUI' on port 4040.
[2025-02-16T14:48:51.797+0000] {spark_submit.py:495} INFO - 25/02/16 14:48:51 INFO StandaloneAppClient$ClientEndpoint: Connecting to master spark://deproject-spark-master:7077...
[2025-02-16T14:48:51.830+0000] {spark_submit.py:495} INFO - 25/02/16 14:48:51 INFO TransportClientFactory: Successfully created connection to deproject-spark-master/172.28.0.3:7077 after 18 ms (0 ms spent in bootstraps)
[2025-02-16T14:48:51.902+0000] {spark_submit.py:495} INFO - 25/02/16 14:48:51 INFO StandaloneSchedulerBackend: Connected to Spark cluster with app ID app-20250216144851-0001
[2025-02-16T14:48:51.905+0000] {spark_submit.py:495} INFO - 25/02/16 14:48:51 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20250216144851-0001/0 on worker-20250216134636-172.28.0.4-41449 (172.28.0.4:41449) with 1 core(s)
[2025-02-16T14:48:51.907+0000] {spark_submit.py:495} INFO - 25/02/16 14:48:51 INFO StandaloneSchedulerBackend: Granted executor ID app-20250216144851-0001/0 on hostPort 172.28.0.4:41449 with 1 core(s), 1024.0 MiB RAM
[2025-02-16T14:48:51.908+0000] {spark_submit.py:495} INFO - 25/02/16 14:48:51 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 42123.
[2025-02-16T14:48:51.909+0000] {spark_submit.py:495} INFO - 25/02/16 14:48:51 INFO NettyBlockTransferService: Server created on deproject-airflow-scheduler:42123
[2025-02-16T14:48:51.910+0000] {spark_submit.py:495} INFO - 25/02/16 14:48:51 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[2025-02-16T14:48:51.914+0000] {spark_submit.py:495} INFO - 25/02/16 14:48:51 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, deproject-airflow-scheduler, 42123, None)
[2025-02-16T14:48:51.917+0000] {spark_submit.py:495} INFO - 25/02/16 14:48:51 INFO BlockManagerMasterEndpoint: Registering block manager deproject-airflow-scheduler:42123 with 434.4 MiB RAM, BlockManagerId(driver, deproject-airflow-scheduler, 42123, None)
[2025-02-16T14:48:51.919+0000] {spark_submit.py:495} INFO - 25/02/16 14:48:51 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, deproject-airflow-scheduler, 42123, None)
[2025-02-16T14:48:51.920+0000] {spark_submit.py:495} INFO - 25/02/16 14:48:51 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, deproject-airflow-scheduler, 42123, None)
[2025-02-16T14:48:51.975+0000] {spark_submit.py:495} INFO - 25/02/16 14:48:51 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20250216144851-0001/0 is now RUNNING
[2025-02-16T14:48:52.078+0000] {spark_submit.py:495} INFO - 25/02/16 14:48:52 INFO StandaloneSchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.0
[2025-02-16T14:48:56.254+0000] {spark_submit.py:495} INFO - +---+--------------------+-------------------+--------------------+--------------------+------+
[2025-02-16T14:48:56.255+0000] {spark_submit.py:495} INFO - | id|                 url|           datetime|               title|                text|source|
[2025-02-16T14:48:56.255+0000] {spark_submit.py:495} INFO - +---+--------------------+-------------------+--------------------+--------------------+------+
[2025-02-16T14:48:56.256+0000] {spark_submit.py:495} INFO - |  0|https://edition.c...|2025-02-14 16:42:00|Drama over Adams ...|Call it the Thurs...|   cnn|
[2025-02-16T14:48:56.256+0000] {spark_submit.py:495} INFO - |  1|https://edition.c...|2025-02-13 21:50:00|Americans voted f...|A version of this...|   cnn|
[2025-02-16T14:48:56.257+0000] {spark_submit.py:495} INFO - |  2|https://edition.c...|2025-02-15 22:00:00|Your questions ab...|A version of this...|   cnn|
[2025-02-16T14:48:56.257+0000] {spark_submit.py:495} INFO - |  3|https://edition.c...|2025-02-14 17:30:00|Thousands of prob...|The Trump adminis...|   cnn|
[2025-02-16T14:48:56.257+0000] {spark_submit.py:495} INFO - |  4|https://edition.c...|2025-02-16 18:19:00|Video: Roy Wood J...|1. How relevant i...|   cnn|
[2025-02-16T14:48:56.258+0000] {spark_submit.py:495} INFO - +---+--------------------+-------------------+--------------------+--------------------+------+
[2025-02-16T14:48:56.258+0000] {spark_submit.py:495} INFO - only showing top 5 rows
[2025-02-16T14:48:56.258+0000] {spark_submit.py:495} INFO - 
[2025-02-16T14:48:56.259+0000] {spark_submit.py:495} INFO - root
[2025-02-16T14:48:56.259+0000] {spark_submit.py:495} INFO - |-- id: integer (nullable = true)
[2025-02-16T14:48:56.259+0000] {spark_submit.py:495} INFO - |-- url: string (nullable = true)
[2025-02-16T14:48:56.260+0000] {spark_submit.py:495} INFO - |-- datetime: timestamp (nullable = true)
[2025-02-16T14:48:56.260+0000] {spark_submit.py:495} INFO - |-- title: string (nullable = true)
[2025-02-16T14:48:56.261+0000] {spark_submit.py:495} INFO - |-- text: string (nullable = true)
[2025-02-16T14:48:56.261+0000] {spark_submit.py:495} INFO - |-- source: string (nullable = true)
[2025-02-16T14:48:56.261+0000] {spark_submit.py:495} INFO - 
[2025-02-16T14:48:56.948+0000] {spark_submit.py:495} INFO - 94
[2025-02-16T14:48:57.259+0000] {spark_submit.py:495} INFO - +---+--------------------+-------------------+--------------------+--------------------+------+
[2025-02-16T14:48:57.260+0000] {spark_submit.py:495} INFO - | id|                 url|           datetime|               title|                text|source|
[2025-02-16T14:48:57.261+0000] {spark_submit.py:495} INFO - +---+--------------------+-------------------+--------------------+--------------------+------+
[2025-02-16T14:48:57.261+0000] {spark_submit.py:495} INFO - |  0|https://edition.c...|2025-02-14 16:42:00|Drama over Adams ...|Call it the Thurs...|   cnn|
[2025-02-16T14:48:57.262+0000] {spark_submit.py:495} INFO - |  1|https://edition.c...|2025-02-13 21:50:00|Americans voted f...|A version of this...|   cnn|
[2025-02-16T14:48:57.262+0000] {spark_submit.py:495} INFO - |  2|https://edition.c...|2025-02-15 22:00:00|Your questions ab...|A version of this...|   cnn|
[2025-02-16T14:48:57.262+0000] {spark_submit.py:495} INFO - |  3|https://edition.c...|2025-02-14 17:30:00|Thousands of prob...|The Trump adminis...|   cnn|
[2025-02-16T14:48:57.263+0000] {spark_submit.py:495} INFO - |  5|https://edition.c...|2025-02-16 20:43:00|Federal appeals c...|A federal appeals...|   cnn|
[2025-02-16T14:48:57.263+0000] {spark_submit.py:495} INFO - +---+--------------------+-------------------+--------------------+--------------------+------+
[2025-02-16T14:48:57.263+0000] {spark_submit.py:495} INFO - only showing top 5 rows
[2025-02-16T14:48:57.264+0000] {spark_submit.py:495} INFO - 
[2025-02-16T14:48:57.614+0000] {spark_submit.py:495} INFO - 82
[2025-02-16T14:48:57.963+0000] {spark_submit.py:495} INFO - 25/02/16 14:48:57 WARN TaskSetManager: Lost task 0.0 in stage 8.0 (TID 6) (172.28.0.4 executor 0): java.io.IOException: Mkdirs failed to create file:/opt/bitnami/spark/data/cnn_spark.csv/_temporary/0/_temporary/attempt_202502161448571160438121475054425_0008_m_000000_6 (exists=false, cwd=file:/opt/bitnami/spark/work/app-20250216144851-0001/0)
[2025-02-16T14:48:57.964+0000] {spark_submit.py:495} INFO - at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:515)
[2025-02-16T14:48:57.964+0000] {spark_submit.py:495} INFO - at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500)
[2025-02-16T14:48:57.965+0000] {spark_submit.py:495} INFO - at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)
[2025-02-16T14:48:57.965+0000] {spark_submit.py:495} INFO - at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1175)
[2025-02-16T14:48:57.965+0000] {spark_submit.py:495} INFO - at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1064)
[2025-02-16T14:48:57.966+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.datasources.CodecStreams$.createOutputStream(CodecStreams.scala:81)
[2025-02-16T14:48:57.966+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.datasources.CodecStreams$.createOutputStreamWriter(CodecStreams.scala:92)
[2025-02-16T14:48:57.966+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.datasources.csv.CsvOutputWriter.<init>(CsvOutputWriter.scala:38)
[2025-02-16T14:48:57.967+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.datasources.csv.CSVFileFormat$$anon$1.newInstance(CSVFileFormat.scala:84)
[2025-02-16T14:48:57.967+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.newOutputWriter(FileFormatDataWriter.scala:161)
[2025-02-16T14:48:57.967+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.<init>(FileFormatDataWriter.scala:146)
[2025-02-16T14:48:57.968+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:327)
[2025-02-16T14:48:57.968+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$22(FileFormatWriter.scala:266)
[2025-02-16T14:48:57.968+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
[2025-02-16T14:48:57.969+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.Task.run(Task.scala:136)
[2025-02-16T14:48:57.969+0000] {spark_submit.py:495} INFO - at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)
[2025-02-16T14:48:57.969+0000] {spark_submit.py:495} INFO - at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)
[2025-02-16T14:48:57.969+0000] {spark_submit.py:495} INFO - at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)
[2025-02-16T14:48:57.970+0000] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
[2025-02-16T14:48:57.970+0000] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
[2025-02-16T14:48:57.970+0000] {spark_submit.py:495} INFO - at java.base/java.lang.Thread.run(Thread.java:833)
[2025-02-16T14:48:57.971+0000] {spark_submit.py:495} INFO - 
[2025-02-16T14:48:57.998+0000] {spark_submit.py:495} INFO - 25/02/16 14:48:57 WARN TaskSetManager: Lost task 0.1 in stage 8.0 (TID 7) (172.28.0.4 executor 0): java.io.IOException: Mkdirs failed to create file:/opt/bitnami/spark/data/cnn_spark.csv/_temporary/0/_temporary/attempt_202502161448571160438121475054425_0008_m_000000_7 (exists=false, cwd=file:/opt/bitnami/spark/work/app-20250216144851-0001/0)
[2025-02-16T14:48:57.999+0000] {spark_submit.py:495} INFO - at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:515)
[2025-02-16T14:48:57.999+0000] {spark_submit.py:495} INFO - at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500)
[2025-02-16T14:48:58.000+0000] {spark_submit.py:495} INFO - at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)
[2025-02-16T14:48:58.000+0000] {spark_submit.py:495} INFO - at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1175)
[2025-02-16T14:48:58.000+0000] {spark_submit.py:495} INFO - at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1064)
[2025-02-16T14:48:58.001+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.datasources.CodecStreams$.createOutputStream(CodecStreams.scala:81)
[2025-02-16T14:48:58.001+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.datasources.CodecStreams$.createOutputStreamWriter(CodecStreams.scala:92)
[2025-02-16T14:48:58.001+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.datasources.csv.CsvOutputWriter.<init>(CsvOutputWriter.scala:38)
[2025-02-16T14:48:58.002+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.datasources.csv.CSVFileFormat$$anon$1.newInstance(CSVFileFormat.scala:84)
[2025-02-16T14:48:58.002+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.newOutputWriter(FileFormatDataWriter.scala:161)
[2025-02-16T14:48:58.002+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.<init>(FileFormatDataWriter.scala:146)
[2025-02-16T14:48:58.003+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:327)
[2025-02-16T14:48:58.003+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$22(FileFormatWriter.scala:266)
[2025-02-16T14:48:58.003+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
[2025-02-16T14:48:58.004+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.Task.run(Task.scala:136)
[2025-02-16T14:48:58.004+0000] {spark_submit.py:495} INFO - at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)
[2025-02-16T14:48:58.004+0000] {spark_submit.py:495} INFO - at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)
[2025-02-16T14:48:58.005+0000] {spark_submit.py:495} INFO - at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)
[2025-02-16T14:48:58.005+0000] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
[2025-02-16T14:48:58.005+0000] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
[2025-02-16T14:48:58.006+0000] {spark_submit.py:495} INFO - at java.base/java.lang.Thread.run(Thread.java:833)
[2025-02-16T14:48:58.006+0000] {spark_submit.py:495} INFO - 
[2025-02-16T14:48:58.035+0000] {spark_submit.py:495} INFO - 25/02/16 14:48:58 WARN TaskSetManager: Lost task 0.2 in stage 8.0 (TID 8) (172.28.0.4 executor 0): java.io.IOException: Mkdirs failed to create file:/opt/bitnami/spark/data/cnn_spark.csv/_temporary/0/_temporary/attempt_202502161448571160438121475054425_0008_m_000000_8 (exists=false, cwd=file:/opt/bitnami/spark/work/app-20250216144851-0001/0)
[2025-02-16T14:48:58.035+0000] {spark_submit.py:495} INFO - at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:515)
[2025-02-16T14:48:58.036+0000] {spark_submit.py:495} INFO - at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500)
[2025-02-16T14:48:58.036+0000] {spark_submit.py:495} INFO - at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)
[2025-02-16T14:48:58.037+0000] {spark_submit.py:495} INFO - at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1175)
[2025-02-16T14:48:58.037+0000] {spark_submit.py:495} INFO - at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1064)
[2025-02-16T14:48:58.038+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.datasources.CodecStreams$.createOutputStream(CodecStreams.scala:81)
[2025-02-16T14:48:58.038+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.datasources.CodecStreams$.createOutputStreamWriter(CodecStreams.scala:92)
[2025-02-16T14:48:58.038+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.datasources.csv.CsvOutputWriter.<init>(CsvOutputWriter.scala:38)
[2025-02-16T14:48:58.039+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.datasources.csv.CSVFileFormat$$anon$1.newInstance(CSVFileFormat.scala:84)
[2025-02-16T14:48:58.039+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.newOutputWriter(FileFormatDataWriter.scala:161)
[2025-02-16T14:48:58.040+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.<init>(FileFormatDataWriter.scala:146)
[2025-02-16T14:48:58.040+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:327)
[2025-02-16T14:48:58.040+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$22(FileFormatWriter.scala:266)
[2025-02-16T14:48:58.041+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
[2025-02-16T14:48:58.041+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.Task.run(Task.scala:136)
[2025-02-16T14:48:58.042+0000] {spark_submit.py:495} INFO - at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)
[2025-02-16T14:48:58.042+0000] {spark_submit.py:495} INFO - at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)
[2025-02-16T14:48:58.042+0000] {spark_submit.py:495} INFO - at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)
[2025-02-16T14:48:58.043+0000] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
[2025-02-16T14:48:58.043+0000] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
[2025-02-16T14:48:58.043+0000] {spark_submit.py:495} INFO - at java.base/java.lang.Thread.run(Thread.java:833)
[2025-02-16T14:48:58.044+0000] {spark_submit.py:495} INFO - 
[2025-02-16T14:48:58.079+0000] {spark_submit.py:495} INFO - 25/02/16 14:48:58 WARN TaskSetManager: Lost task 0.3 in stage 8.0 (TID 9) (172.28.0.4 executor 0): java.io.IOException: Mkdirs failed to create file:/opt/bitnami/spark/data/cnn_spark.csv/_temporary/0/_temporary/attempt_202502161448571160438121475054425_0008_m_000000_9 (exists=false, cwd=file:/opt/bitnami/spark/work/app-20250216144851-0001/0)
[2025-02-16T14:48:58.080+0000] {spark_submit.py:495} INFO - at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:515)
[2025-02-16T14:48:58.080+0000] {spark_submit.py:495} INFO - at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500)
[2025-02-16T14:48:58.081+0000] {spark_submit.py:495} INFO - at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)
[2025-02-16T14:48:58.081+0000] {spark_submit.py:495} INFO - at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1175)
[2025-02-16T14:48:58.082+0000] {spark_submit.py:495} INFO - at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1064)
[2025-02-16T14:48:58.082+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.datasources.CodecStreams$.createOutputStream(CodecStreams.scala:81)
[2025-02-16T14:48:58.083+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.datasources.CodecStreams$.createOutputStreamWriter(CodecStreams.scala:92)
[2025-02-16T14:48:58.083+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.datasources.csv.CsvOutputWriter.<init>(CsvOutputWriter.scala:38)
[2025-02-16T14:48:58.084+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.datasources.csv.CSVFileFormat$$anon$1.newInstance(CSVFileFormat.scala:84)
[2025-02-16T14:48:58.084+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.newOutputWriter(FileFormatDataWriter.scala:161)
[2025-02-16T14:48:58.085+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.<init>(FileFormatDataWriter.scala:146)
[2025-02-16T14:48:58.085+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:327)
[2025-02-16T14:48:58.085+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$22(FileFormatWriter.scala:266)
[2025-02-16T14:48:58.086+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
[2025-02-16T14:48:58.086+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.Task.run(Task.scala:136)
[2025-02-16T14:48:58.086+0000] {spark_submit.py:495} INFO - at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)
[2025-02-16T14:48:58.087+0000] {spark_submit.py:495} INFO - at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)
[2025-02-16T14:48:58.087+0000] {spark_submit.py:495} INFO - at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)
[2025-02-16T14:48:58.087+0000] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
[2025-02-16T14:48:58.088+0000] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
[2025-02-16T14:48:58.088+0000] {spark_submit.py:495} INFO - at java.base/java.lang.Thread.run(Thread.java:833)
[2025-02-16T14:48:58.088+0000] {spark_submit.py:495} INFO - 
[2025-02-16T14:48:58.089+0000] {spark_submit.py:495} INFO - 25/02/16 14:48:58 ERROR TaskSetManager: Task 0 in stage 8.0 failed 4 times; aborting job
[2025-02-16T14:48:58.089+0000] {spark_submit.py:495} INFO - 25/02/16 14:48:58 ERROR FileFormatWriter: Aborting job 517eae16-60ae-4512-bd1f-fc2fd89d0949.
[2025-02-16T14:48:58.090+0000] {spark_submit.py:495} INFO - org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 8.0 failed 4 times, most recent failure: Lost task 0.3 in stage 8.0 (TID 9) (172.28.0.4 executor 0): java.io.IOException: Mkdirs failed to create file:/opt/bitnami/spark/data/cnn_spark.csv/_temporary/0/_temporary/attempt_202502161448571160438121475054425_0008_m_000000_9 (exists=false, cwd=file:/opt/bitnami/spark/work/app-20250216144851-0001/0)
[2025-02-16T14:48:58.090+0000] {spark_submit.py:495} INFO - at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:515)
[2025-02-16T14:48:58.090+0000] {spark_submit.py:495} INFO - at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500)
[2025-02-16T14:48:58.091+0000] {spark_submit.py:495} INFO - at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)
[2025-02-16T14:48:58.091+0000] {spark_submit.py:495} INFO - at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1175)
[2025-02-16T14:48:58.091+0000] {spark_submit.py:495} INFO - at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1064)
[2025-02-16T14:48:58.092+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.datasources.CodecStreams$.createOutputStream(CodecStreams.scala:81)
[2025-02-16T14:48:58.092+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.datasources.CodecStreams$.createOutputStreamWriter(CodecStreams.scala:92)
[2025-02-16T14:48:58.092+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.datasources.csv.CsvOutputWriter.<init>(CsvOutputWriter.scala:38)
[2025-02-16T14:48:58.093+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.datasources.csv.CSVFileFormat$$anon$1.newInstance(CSVFileFormat.scala:84)
[2025-02-16T14:48:58.093+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.newOutputWriter(FileFormatDataWriter.scala:161)
[2025-02-16T14:48:58.094+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.<init>(FileFormatDataWriter.scala:146)
[2025-02-16T14:48:58.094+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:327)
[2025-02-16T14:48:58.094+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$22(FileFormatWriter.scala:266)
[2025-02-16T14:48:58.095+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
[2025-02-16T14:48:58.095+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.Task.run(Task.scala:136)
[2025-02-16T14:48:58.096+0000] {spark_submit.py:495} INFO - at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)
[2025-02-16T14:48:58.096+0000] {spark_submit.py:495} INFO - at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)
[2025-02-16T14:48:58.096+0000] {spark_submit.py:495} INFO - at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)
[2025-02-16T14:48:58.096+0000] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
[2025-02-16T14:48:58.097+0000] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
[2025-02-16T14:48:58.097+0000] {spark_submit.py:495} INFO - at java.base/java.lang.Thread.run(Thread.java:833)
[2025-02-16T14:48:58.097+0000] {spark_submit.py:495} INFO - 
[2025-02-16T14:48:58.098+0000] {spark_submit.py:495} INFO - Driver stacktrace:
[2025-02-16T14:48:58.098+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2672)
[2025-02-16T14:48:58.098+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2608)
[2025-02-16T14:48:58.099+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2607)
[2025-02-16T14:48:58.099+0000] {spark_submit.py:495} INFO - at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
[2025-02-16T14:48:58.099+0000] {spark_submit.py:495} INFO - at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
[2025-02-16T14:48:58.100+0000] {spark_submit.py:495} INFO - at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
[2025-02-16T14:48:58.100+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2607)
[2025-02-16T14:48:58.100+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1182)
[2025-02-16T14:48:58.101+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1182)
[2025-02-16T14:48:58.101+0000] {spark_submit.py:495} INFO - at scala.Option.foreach(Option.scala:407)
[2025-02-16T14:48:58.101+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1182)
[2025-02-16T14:48:58.102+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2860)
[2025-02-16T14:48:58.102+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2802)
[2025-02-16T14:48:58.102+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2791)
[2025-02-16T14:48:58.103+0000] {spark_submit.py:495} INFO - at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)
[2025-02-16T14:48:58.103+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:952)
[2025-02-16T14:48:58.103+0000] {spark_submit.py:495} INFO - at org.apache.spark.SparkContext.runJob(SparkContext.scala:2238)
[2025-02-16T14:48:58.104+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:255)
[2025-02-16T14:48:58.104+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:186)
[2025-02-16T14:48:58.104+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:113)
[2025-02-16T14:48:58.105+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:111)
[2025-02-16T14:48:58.105+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:125)
[2025-02-16T14:48:58.105+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:98)
[2025-02-16T14:48:58.106+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:109)
[2025-02-16T14:48:58.106+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:169)
[2025-02-16T14:48:58.107+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:95)
[2025-02-16T14:48:58.107+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)
[2025-02-16T14:48:58.107+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)
[2025-02-16T14:48:58.107+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)
[2025-02-16T14:48:58.108+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:94)
[2025-02-16T14:48:58.108+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:584)
[2025-02-16T14:48:58.108+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:176)
[2025-02-16T14:48:58.109+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:584)
[2025-02-16T14:48:58.109+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:30)
[2025-02-16T14:48:58.109+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)
[2025-02-16T14:48:58.110+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)
[2025-02-16T14:48:58.110+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)
[2025-02-16T14:48:58.110+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)
[2025-02-16T14:48:58.111+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:560)
[2025-02-16T14:48:58.111+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:94)
[2025-02-16T14:48:58.111+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:81)
[2025-02-16T14:48:58.112+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:79)
[2025-02-16T14:48:58.112+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:116)
[2025-02-16T14:48:58.112+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:860)
[2025-02-16T14:48:58.113+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:390)
[2025-02-16T14:48:58.113+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:363)
[2025-02-16T14:48:58.113+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:239)
[2025-02-16T14:48:58.114+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.DataFrameWriter.csv(DataFrameWriter.scala:851)
[2025-02-16T14:48:58.114+0000] {spark_submit.py:495} INFO - at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
[2025-02-16T14:48:58.114+0000] {spark_submit.py:495} INFO - at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)
[2025-02-16T14:48:58.115+0000] {spark_submit.py:495} INFO - at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
[2025-02-16T14:48:58.115+0000] {spark_submit.py:495} INFO - at java.base/java.lang.reflect.Method.invoke(Method.java:569)
[2025-02-16T14:48:58.115+0000] {spark_submit.py:495} INFO - at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
[2025-02-16T14:48:58.116+0000] {spark_submit.py:495} INFO - at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
[2025-02-16T14:48:58.116+0000] {spark_submit.py:495} INFO - at py4j.Gateway.invoke(Gateway.java:282)
[2025-02-16T14:48:58.116+0000] {spark_submit.py:495} INFO - at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
[2025-02-16T14:48:58.117+0000] {spark_submit.py:495} INFO - at py4j.commands.CallCommand.execute(CallCommand.java:79)
[2025-02-16T14:48:58.117+0000] {spark_submit.py:495} INFO - at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
[2025-02-16T14:48:58.117+0000] {spark_submit.py:495} INFO - at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
[2025-02-16T14:48:58.118+0000] {spark_submit.py:495} INFO - at java.base/java.lang.Thread.run(Thread.java:840)
[2025-02-16T14:48:58.118+0000] {spark_submit.py:495} INFO - Caused by: java.io.IOException: Mkdirs failed to create file:/opt/bitnami/spark/data/cnn_spark.csv/_temporary/0/_temporary/attempt_202502161448571160438121475054425_0008_m_000000_9 (exists=false, cwd=file:/opt/bitnami/spark/work/app-20250216144851-0001/0)
[2025-02-16T14:48:58.118+0000] {spark_submit.py:495} INFO - at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:515)
[2025-02-16T14:48:58.119+0000] {spark_submit.py:495} INFO - at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500)
[2025-02-16T14:48:58.119+0000] {spark_submit.py:495} INFO - at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)
[2025-02-16T14:48:58.119+0000] {spark_submit.py:495} INFO - at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1175)
[2025-02-16T14:48:58.119+0000] {spark_submit.py:495} INFO - at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1064)
[2025-02-16T14:48:58.120+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.datasources.CodecStreams$.createOutputStream(CodecStreams.scala:81)
[2025-02-16T14:48:58.120+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.datasources.CodecStreams$.createOutputStreamWriter(CodecStreams.scala:92)
[2025-02-16T14:48:58.121+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.datasources.csv.CsvOutputWriter.<init>(CsvOutputWriter.scala:38)
[2025-02-16T14:48:58.121+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.datasources.csv.CSVFileFormat$$anon$1.newInstance(CSVFileFormat.scala:84)
[2025-02-16T14:48:58.121+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.newOutputWriter(FileFormatDataWriter.scala:161)
[2025-02-16T14:48:58.122+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.<init>(FileFormatDataWriter.scala:146)
[2025-02-16T14:48:58.122+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:327)
[2025-02-16T14:48:58.122+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$22(FileFormatWriter.scala:266)
[2025-02-16T14:48:58.122+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
[2025-02-16T14:48:58.123+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.Task.run(Task.scala:136)
[2025-02-16T14:48:58.123+0000] {spark_submit.py:495} INFO - at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)
[2025-02-16T14:48:58.123+0000] {spark_submit.py:495} INFO - at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)
[2025-02-16T14:48:58.123+0000] {spark_submit.py:495} INFO - at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)
[2025-02-16T14:48:58.124+0000] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
[2025-02-16T14:48:58.124+0000] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
[2025-02-16T14:48:58.124+0000] {spark_submit.py:495} INFO - at java.base/java.lang.Thread.run(Thread.java:833)
[2025-02-16T14:48:58.243+0000] {spark_submit.py:495} INFO - Traceback (most recent call last):
[2025-02-16T14:48:58.243+0000] {spark_submit.py:495} INFO - File "/spark-scripts/spark-news.py", line 34, in <module>
[2025-02-16T14:48:58.244+0000] {spark_submit.py:495} INFO - df.write.csv('/opt/bitnami/spark/data/cnn_spark.csv')
[2025-02-16T14:48:58.245+0000] {spark_submit.py:495} INFO - File "/home/airflow/.local/lib/python3.9/site-packages/pyspark/python/lib/pyspark.zip/pyspark/sql/readwriter.py", line 1240, in csv
[2025-02-16T14:48:58.245+0000] {spark_submit.py:495} INFO - File "/home/airflow/.local/lib/python3.9/site-packages/pyspark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py", line 1321, in __call__
[2025-02-16T14:48:58.245+0000] {spark_submit.py:495} INFO - File "/home/airflow/.local/lib/python3.9/site-packages/pyspark/python/lib/pyspark.zip/pyspark/sql/utils.py", line 190, in deco
[2025-02-16T14:48:58.245+0000] {spark_submit.py:495} INFO - File "/home/airflow/.local/lib/python3.9/site-packages/pyspark/python/lib/py4j-0.10.9.5-src.zip/py4j/protocol.py", line 326, in get_return_value
[2025-02-16T14:48:58.291+0000] {spark_submit.py:495} INFO - py4j.protocol.Py4JJavaError: An error occurred while calling o45.csv.
[2025-02-16T14:48:58.292+0000] {spark_submit.py:495} INFO - : org.apache.spark.SparkException: Job aborted.
[2025-02-16T14:48:58.293+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.errors.QueryExecutionErrors$.jobAbortedError(QueryExecutionErrors.scala:651)
[2025-02-16T14:48:58.293+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:288)
[2025-02-16T14:48:58.294+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:186)
[2025-02-16T14:48:58.294+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:113)
[2025-02-16T14:48:58.294+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:111)
[2025-02-16T14:48:58.294+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:125)
[2025-02-16T14:48:58.295+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:98)
[2025-02-16T14:48:58.295+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:109)
[2025-02-16T14:48:58.295+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:169)
[2025-02-16T14:48:58.296+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:95)
[2025-02-16T14:48:58.296+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)
[2025-02-16T14:48:58.296+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)
[2025-02-16T14:48:58.297+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)
[2025-02-16T14:48:58.297+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:94)
[2025-02-16T14:48:58.297+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:584)
[2025-02-16T14:48:58.298+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:176)
[2025-02-16T14:48:58.298+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:584)
[2025-02-16T14:48:58.298+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:30)
[2025-02-16T14:48:58.299+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)
[2025-02-16T14:48:58.299+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)
[2025-02-16T14:48:58.299+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)
[2025-02-16T14:48:58.299+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)
[2025-02-16T14:48:58.300+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:560)
[2025-02-16T14:48:58.300+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:94)
[2025-02-16T14:48:58.300+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:81)
[2025-02-16T14:48:58.300+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:79)
[2025-02-16T14:48:58.301+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:116)
[2025-02-16T14:48:58.301+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:860)
[2025-02-16T14:48:58.301+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:390)
[2025-02-16T14:48:58.302+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:363)
[2025-02-16T14:48:58.302+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:239)
[2025-02-16T14:48:58.302+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.DataFrameWriter.csv(DataFrameWriter.scala:851)
[2025-02-16T14:48:58.303+0000] {spark_submit.py:495} INFO - at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
[2025-02-16T14:48:58.303+0000] {spark_submit.py:495} INFO - at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)
[2025-02-16T14:48:58.303+0000] {spark_submit.py:495} INFO - at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
[2025-02-16T14:48:58.303+0000] {spark_submit.py:495} INFO - at java.base/java.lang.reflect.Method.invoke(Method.java:569)
[2025-02-16T14:48:58.304+0000] {spark_submit.py:495} INFO - at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
[2025-02-16T14:48:58.304+0000] {spark_submit.py:495} INFO - at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
[2025-02-16T14:48:58.305+0000] {spark_submit.py:495} INFO - at py4j.Gateway.invoke(Gateway.java:282)
[2025-02-16T14:48:58.305+0000] {spark_submit.py:495} INFO - at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
[2025-02-16T14:48:58.305+0000] {spark_submit.py:495} INFO - at py4j.commands.CallCommand.execute(CallCommand.java:79)
[2025-02-16T14:48:58.306+0000] {spark_submit.py:495} INFO - at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
[2025-02-16T14:48:58.306+0000] {spark_submit.py:495} INFO - at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
[2025-02-16T14:48:58.306+0000] {spark_submit.py:495} INFO - at java.base/java.lang.Thread.run(Thread.java:840)
[2025-02-16T14:48:58.306+0000] {spark_submit.py:495} INFO - Caused by: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 8.0 failed 4 times, most recent failure: Lost task 0.3 in stage 8.0 (TID 9) (172.28.0.4 executor 0): java.io.IOException: Mkdirs failed to create file:/opt/bitnami/spark/data/cnn_spark.csv/_temporary/0/_temporary/attempt_202502161448571160438121475054425_0008_m_000000_9 (exists=false, cwd=file:/opt/bitnami/spark/work/app-20250216144851-0001/0)
[2025-02-16T14:48:58.307+0000] {spark_submit.py:495} INFO - at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:515)
[2025-02-16T14:48:58.307+0000] {spark_submit.py:495} INFO - at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500)
[2025-02-16T14:48:58.307+0000] {spark_submit.py:495} INFO - at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)
[2025-02-16T14:48:58.308+0000] {spark_submit.py:495} INFO - at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1175)
[2025-02-16T14:48:58.308+0000] {spark_submit.py:495} INFO - at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1064)
[2025-02-16T14:48:58.308+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.datasources.CodecStreams$.createOutputStream(CodecStreams.scala:81)
[2025-02-16T14:48:58.308+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.datasources.CodecStreams$.createOutputStreamWriter(CodecStreams.scala:92)
[2025-02-16T14:48:58.309+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.datasources.csv.CsvOutputWriter.<init>(CsvOutputWriter.scala:38)
[2025-02-16T14:48:58.309+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.datasources.csv.CSVFileFormat$$anon$1.newInstance(CSVFileFormat.scala:84)
[2025-02-16T14:48:58.309+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.newOutputWriter(FileFormatDataWriter.scala:161)
[2025-02-16T14:48:58.309+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.<init>(FileFormatDataWriter.scala:146)
[2025-02-16T14:48:58.310+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:327)
[2025-02-16T14:48:58.310+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$22(FileFormatWriter.scala:266)
[2025-02-16T14:48:58.310+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
[2025-02-16T14:48:58.310+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.Task.run(Task.scala:136)
[2025-02-16T14:48:58.311+0000] {spark_submit.py:495} INFO - at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)
[2025-02-16T14:48:58.311+0000] {spark_submit.py:495} INFO - at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)
[2025-02-16T14:48:58.311+0000] {spark_submit.py:495} INFO - at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)
[2025-02-16T14:48:58.311+0000] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
[2025-02-16T14:48:58.312+0000] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
[2025-02-16T14:48:58.312+0000] {spark_submit.py:495} INFO - at java.base/java.lang.Thread.run(Thread.java:833)
[2025-02-16T14:48:58.312+0000] {spark_submit.py:495} INFO - 
[2025-02-16T14:48:58.312+0000] {spark_submit.py:495} INFO - Driver stacktrace:
[2025-02-16T14:48:58.313+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2672)
[2025-02-16T14:48:58.313+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2608)
[2025-02-16T14:48:58.313+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2607)
[2025-02-16T14:48:58.314+0000] {spark_submit.py:495} INFO - at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
[2025-02-16T14:48:58.314+0000] {spark_submit.py:495} INFO - at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
[2025-02-16T14:48:58.314+0000] {spark_submit.py:495} INFO - at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
[2025-02-16T14:48:58.314+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2607)
[2025-02-16T14:48:58.315+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1182)
[2025-02-16T14:48:58.315+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1182)
[2025-02-16T14:48:58.315+0000] {spark_submit.py:495} INFO - at scala.Option.foreach(Option.scala:407)
[2025-02-16T14:48:58.315+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1182)
[2025-02-16T14:48:58.316+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2860)
[2025-02-16T14:48:58.316+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2802)
[2025-02-16T14:48:58.316+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2791)
[2025-02-16T14:48:58.317+0000] {spark_submit.py:495} INFO - at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)
[2025-02-16T14:48:58.317+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:952)
[2025-02-16T14:48:58.317+0000] {spark_submit.py:495} INFO - at org.apache.spark.SparkContext.runJob(SparkContext.scala:2238)
[2025-02-16T14:48:58.317+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:255)
[2025-02-16T14:48:58.318+0000] {spark_submit.py:495} INFO - ... 42 more
[2025-02-16T14:48:58.318+0000] {spark_submit.py:495} INFO - Caused by: java.io.IOException: Mkdirs failed to create file:/opt/bitnami/spark/data/cnn_spark.csv/_temporary/0/_temporary/attempt_202502161448571160438121475054425_0008_m_000000_9 (exists=false, cwd=file:/opt/bitnami/spark/work/app-20250216144851-0001/0)
[2025-02-16T14:48:58.318+0000] {spark_submit.py:495} INFO - at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:515)
[2025-02-16T14:48:58.319+0000] {spark_submit.py:495} INFO - at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500)
[2025-02-16T14:48:58.319+0000] {spark_submit.py:495} INFO - at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)
[2025-02-16T14:48:58.319+0000] {spark_submit.py:495} INFO - at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1175)
[2025-02-16T14:48:58.319+0000] {spark_submit.py:495} INFO - at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1064)
[2025-02-16T14:48:58.319+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.datasources.CodecStreams$.createOutputStream(CodecStreams.scala:81)
[2025-02-16T14:48:58.320+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.datasources.CodecStreams$.createOutputStreamWriter(CodecStreams.scala:92)
[2025-02-16T14:48:58.320+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.datasources.csv.CsvOutputWriter.<init>(CsvOutputWriter.scala:38)
[2025-02-16T14:48:58.320+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.datasources.csv.CSVFileFormat$$anon$1.newInstance(CSVFileFormat.scala:84)
[2025-02-16T14:48:58.320+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.newOutputWriter(FileFormatDataWriter.scala:161)
[2025-02-16T14:48:58.321+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.<init>(FileFormatDataWriter.scala:146)
[2025-02-16T14:48:58.321+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:327)
[2025-02-16T14:48:58.321+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$22(FileFormatWriter.scala:266)
[2025-02-16T14:48:58.321+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
[2025-02-16T14:48:58.322+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.Task.run(Task.scala:136)
[2025-02-16T14:48:58.322+0000] {spark_submit.py:495} INFO - at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)
[2025-02-16T14:48:58.322+0000] {spark_submit.py:495} INFO - at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)
[2025-02-16T14:48:58.323+0000] {spark_submit.py:495} INFO - at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)
[2025-02-16T14:48:58.323+0000] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
[2025-02-16T14:48:58.323+0000] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
[2025-02-16T14:48:58.323+0000] {spark_submit.py:495} INFO - at java.base/java.lang.Thread.run(Thread.java:833)
[2025-02-16T14:48:58.324+0000] {spark_submit.py:495} INFO - 
[2025-02-16T14:48:58.429+0000] {taskinstance.py:2728} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/taskinstance.py", line 444, in _execute_task
    result = _execute_callable(context=context, **execute_callable_kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/taskinstance.py", line 414, in _execute_callable
    return execute_callable(context=context, **execute_callable_kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/providers/apache/spark/operators/spark_submit.py", line 157, in execute
    self._hook.submit(self._application)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/providers/apache/spark/hooks/spark_submit.py", line 426, in submit
    raise AirflowException(
airflow.exceptions.AirflowException: Cannot execute: spark-submit --master spark://deproject-spark-master:7077 --name arrow-spark /spark-scripts/spark-news.py. Error code is: 1.
[2025-02-16T14:48:58.432+0000] {taskinstance.py:1149} INFO - Marking task as FAILED. dag_id=scraping_news_dag, task_id=spark_load_task, execution_date=20250216T144610, start_date=20250216T144848, end_date=20250216T144858
[2025-02-16T14:48:58.446+0000] {standard_task_runner.py:107} ERROR - Failed to execute job 98 for task spark_load_task (Cannot execute: spark-submit --master spark://deproject-spark-master:7077 --name arrow-spark /spark-scripts/spark-news.py. Error code is: 1.; 3487)
[2025-02-16T14:48:58.469+0000] {local_task_job_runner.py:234} INFO - Task exited with return code 1
[2025-02-16T14:48:58.489+0000] {taskinstance.py:3309} INFO - 0 downstream tasks scheduled from follow-on schedule check
